---
title: "Titanic ML"
author: "Timothy Ripper"
format: html
editor: visual
---

```{r}
#libraries
library(readxl)
library(ggplot2)
library(caret)
library(MASS)
library(dslabs)
library(tidyverse)
library(randomForest)
library(Rborist)
```

## Section 1: Loading the data (5 points)

```{r}

test_base <- read.csv("C:/Users/timot/Downloads/test.csv")
gender_submission <- read.csv("C:/Users/timot/Downloads/gender_submission.csv")
train_base <- read.csv("C:/Users/timot/Downloads/train.csv")

View(gender_submission)
View(test_base)
View(train_base)
```

##  Section 2: Descriptive Statistics (including charts); include any discussion of contextual knowledge (15 points)

Our goal here is to get a feel for the data - what trends we should expect to see.

First thing to check is just how many people survived - what our baseline guess could be if we say that everyone either survived or died, and try to beat that number.

```{r}
train_base %>%
  count(Survived) %>%
ggplot(aes(x = Survived, y = n, fill = factor(Survived))) +
  geom_bar(stat = 'identity')
```

In general, we should expect about half again as many people to die as to survive.

Second, survival rates by sex.

```{r}
train_base %>%
  group_by(Sex, Survived) %>%
  summarise(N = n()) %>%
  ggplot(aes(x = Sex, y = N, fill = factor(Survived))) +
  geom_bar(stat = 'identity')
```

This is not surprising that about 75% of women survive and about 15% of men survive.

Third, survival rates by age.

```{r}
ggplot(train_base, aes(x = Age, fill = factor(Survived)), group = factor(Survived)) +
  geom_histogram(color = "blue", binwidth = 5)
```

The info here is more spotty but very young seem to survive more often than older ones.

Fourth, survival rates by class.

```{r}
train_base %>%
  group_by(Pclass, Survived) %>%
  summarise(N = n()) %>%
  ggplot(aes(x = Pclass, y = N, fill = factor(Survived))) +
  geom_bar(stat = 'identity')
```

Good data here. 1st and 2nd class do pretty well, and steerage does pretty poorly.

I want to do a quick check to see if fare rates and class are about the same.

```{r}
ggplot(train_base, aes(x = Fare, fill = factor(Survived)), group = factor(Survived)) +
  geom_histogram(color = "blue")
```

This looks like a pretty clear showing that people who paid higher fares are more likely to survive, but there's some outliers and the more expensive tickets don't have as many data points so that may not come up as significant in a stepwise.

Last, I'm curious about embarking location and survival.

```{r}
train_base %>%
  group_by(Embarked, Survived) %>%
  summarise(N = n()) %>%
  ggplot(aes(x = Embarked, y = N, fill = factor(Survived))) +
  geom_bar(stat = 'identity')
```

Nothing too crazy here, maybe C is a little better than the others but nothing that is really shocking.

My initial guess is that women in 1st and 2nd class, and their children around age 10 have the highest rate of survival, and that men in steerage have the worst rates.

##  Section 3: Feature Engineering (if any) (10 points)

Mother and Father flags.

```{r}
train_base <- train_basea %>% 
  mutate(Mother = ifelse(Age > 18 & Sex =="female" & Parch >= 1, "Mother", "Not"))
train_base <- train_base %>% 
  mutate(Father = ifelse(Age > 18 & Sex =="male" & Parch >= 1, "Father", "Not"))
```

Unaccompanied children and lone travelers

```{r}
train_base <- train_base %>% 
  mutate(LoneChild = ifelse(Age < 18 & Parch == 0, "LoneChild", "Not"))
train_base <- train_base %>%
  mutate(LoneTravel = ifelse(Parch == 0 & SipSp == 0, "LoneTravel", "Not"))
```

Unmarried adults without children.

```{r}
train_base <- train_base %>% 
  mutate(Unmarried = ifelse(Age > 18 & Parch == 0 & SipSp == 0, "Unmarried", "Not"))
```

Total family size.

```{r}
train_base <- train_base %>%
  mutate(FamilySize = SibSp + Parch + 1)
```

This is similar to the other values but I'm curious if overall family size will give a different result.

##  Section 4: Dealing with Missing Data (10 points)

Impute age by averaging NAs.

```{r} train_base$Age[is.na(train_base$Age)] <- mean(train_basea$Age, na.rm = TRUE)}
```

We will remove cabin number because I don't care for it as a data point and it makes the models easier to code when I don't have to type out each field I need to put in it.

```{r}

```

## 
 Section 5: Feature Selection (10 points)

```{r}

```

#determining which predictors should be included in a model is becoming one of the most critical questions as data are increasingly high-dimensional

#remove predictors that are not informative

#remove predictors that are highly correlated with other predictors

#some models are naturally resistant to non-informative predictors like trees, rule-based models, and MARS (multivariate adaptive regression splines)

##  Section 6: Comparing Models & Tuning parameters (you can have subsections if you’d like) (25 points; each unique model is worth 5 points – you’ll need to do 5 to get full credit; you can’t get “extra credit” by doing more)

Create our test index and training/testing set for the models.

```{r}
test_index <- createDataPartition(train_base$Survived, times = 1, p=0.5, list=FALSE)
test_set <- train_base[test_index,]
train_set <- train_base[-test_index,]
```

Doing a stepwise analysis to see what features are working for us.

```{r}
initial <- glm(Survived ~ Age + factor(Pclass) + Sex + SibSp + Parch + Fare, data = train_set, family = binomial)
stepAIC(initial, direction = "both")
```

#### GLM

```{r}
```

#### 

#### KNN

```{r}

```

#### Decision Tree

```{r}

```

#### Random Forest

```{r}

```

#### QDA

```{r}

```

##  Section 7: Prediction (write your solution to a csv file) (5 points) Be sure to explain what you do in each step so that someone else reading your code could follow what you did (15 points)

#### GLM

```{r} survived_test <- (predict(fit, test_base, type = "response")) submission <- test_base %>%   select(PassengerId) %>%   mutate(Survived = factor(ifelse(survived_test > 0.5, 1, 0))) write.csv(submission, 'submission.csv', row.names = FALSE)}
```

\
KNN

```{r}
survived_test <- (predict(fit, test_base, type = "response"))
submission <- test_base %>%
  select(PassengerId) %>%
  mutate(Survived = factor(ifelse(survived_test > 0.5, 1, 0)))
write.csv(submission, 'submission.csv', row.names = FALSE)
```

#### Decision Tree

```{r} survived_test <- (predict(fit, test_base, type = "response")) submission <- test_base %>%   select(PassengerId) %>%   mutate(Survived = factor(ifelse(survived_test > 0.5, 1, 0))) write.csv(submission, 'submission.csv', row.names = FALSE)}
```

#### Random Forest

```{r} survived_test <- (predict(fit, test_base, type = "response")) submission <- test_base %>%   select(PassengerId) %>%   mutate(Survived = factor(ifelse(survived_test > 0.5, 1, 0))) write.csv(submission, 'submission.csv', row.names = FALSE)}
```

#### QDA

```{r} survived_test <- (predict(fit, test_base, type = "response")) submission <- test_base %>%   select(PassengerId) %>%   mutate(Survived = factor(ifelse(survived_test > 0.5, 1, 0))) write.csv(submission, 'submission.csv', row.names = FALSE)}
```

#### Ensemble

```{r}

```
